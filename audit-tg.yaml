AWSTemplateFormatVersion: 2010-09-09
Description: "AWS Lambda function for audit ALB Target Groups"
Parameters:
  Prefix:
    Description: 'Prefix to distinguish all names as part of this app'
    AllowedPattern: '^[a-z\-0-9]*$'
    Type: 'String'
    Default: 'devops'
  Environment:
    Description: 'Environment belonging to the scope'
    AllowedPattern: '^[a-z\-0-9]*$'
    Type: 'String'
    Default: 'dev'
Resources:
  AuditTGS3Bucket:
    Type: 'AWS::S3::Bucket'
    Properties:
      BucketName: !Sub '${Prefix}-${Environment}-backups'
      BucketEncryption:
        ServerSideEncryptionConfiguration:
        - ServerSideEncryptionByDefault:
            SSEAlgorithm: AES256
    DeletionPolicy: 'Retain'
  AuditTGExecutionRole:
    Type: AWS::IAM::Role
    Properties:
      Description: Role for Lambda 
      RoleName: !Sub "${Prefix}-${Environment}-role"
      Path: '/'
      AssumeRolePolicyDocument:
        Version: 2012-10-17
        Statement:
         - Effect: Allow
           Principal:
             Service:
              - lambda.amazonaws.com
           Action:
            - 'sts:AssumeRole'
      Policies:
        - PolicyName: lambda_execution_policy
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - 's3:PutObject*'
                  - 's3:ListBucket*'
                Resource:
                  - !Sub ${AuditTGS3Bucket.Arn}/*
              - Effect: Allow
                Action:
                  - 'elasticloadbalancing:Describe*'
                Resource: '*'
              - Effect: Allow
                Action:
                  - 'logs:CreateLogStream'
                  - 'logs:PutLogEvents'
                Resource: !Sub arn:aws:logs:*:*:log-group:/aws/lambda/${AWS::StackName}*:*
      
  AuditTGFunction:
    Type: AWS::Lambda::Function
    Properties:
      Code:
        ZipFile: |
          import boto3
          import csv
          import os.path
          from datetime import date
          
          client = boto3.client('elbv2')
          s3_resource = boto3.resource('s3')

          bucket_name = "devops-dev-backups"
          tmp_path = '/tmp/'
          filename = 'result-' + str(date.today()) + '.csv'
          full_file_path = os.path.join(tmp_path, filename)
          
          
          def get_target_groups_data() -> list:
              target_group_data = list()
              response = client.describe_target_groups()
              if response['ResponseMetadata']['HTTPStatusCode'] == 200:
                  for tg in response['TargetGroups']:
                      target_group_data.append({'TargetGroupName': tg['TargetGroupName'],
                                                'TargetGroupArn': tg['TargetGroupArn'],
                                                'LoadBalancerArnsList': tg['LoadBalancerArns'],
                                                'Port': tg['Port']})
                  if response.get('NextMarker'):
                      current_marker = response['NextMarker']
                      while True:
                          response_with_marker = client.describe_target_groups(Marker=current_marker)
                          if response_with_marker['ResponseMetadata']['HTTPStatusCode'] == 200:
                              for tg in response_with_marker['TargetGroups']:
                                  target_group_data.append({'TargetGroupName': tg['TargetGroupName'],
                                                            'TargetGroupArn': tg['TargetGroupArn'],
                                                            'LoadBalancerArnsList': tg['LoadBalancerArns'],
                                                            'Port': tg['Port']})
                              current_marker = response_with_marker.get('NextMarker')
                              if current_marker:
                                  pass
                              else:
                                  break 
                          else:
                              print(f"Additional request with next results marker failed"
                                    f" with code:{response['ResponseMetadata']['HTTPStatusCode']}")
                              break
                      return target_group_data
                  else:
                      return target_group_data
              else:
                  print(f"Request to describe_target_groups failed "
                        f"with code:{response['ResponseMetadata']['HTTPStatusCode']}")

          def fill_instances_ids(target_group_data: list) -> list:
              result_list = []
              for tg_dict in target_group_data:
                  response = client.describe_target_health(TargetGroupArn=tg_dict['TargetGroupArn'])
                  if response['ResponseMetadata']['HTTPStatusCode'] == 200:
                      for target in response['TargetHealthDescriptions']:
                          result_list.append({'TargetGroupName': tg_dict['TargetGroupName'],
                                              'TargetGroupArn': tg_dict['TargetGroupArn'],
                                              'LoadBalancerArnsList': tg_dict['LoadBalancerArnsList'],
                                              'Port': tg_dict['Port'],
                                              'InstanceId': target['Target']['Id'],
                                              'TargetPort': target['Target']['Port']})
                  else:
                      print(f"Request to describe_target_health failed "
                            f"with code:{response['ResponseMetadata']['HTTPStatusCode']}")
              return result_list

          def save_to_csv(data, full_file_path):
              with open(full_file_path, 'w', encoding='utf8', newline='') as output_file:
                  fc = csv.DictWriter(output_file,
                                      fieldnames=data[0].keys(),
                                      )
                  fc.writeheader()
                  fc.writerows(data)
          
          def save_to_bucket(bucket_name, filename):
              s3_resource.meta.client.upload_file(
                  Filename=full_file_path, Bucket=bucket_name,
                  Key=filename)
          
          def create_bucket(bucket_name):
              s3_resource.create_bucket(Bucket=bucket_name)
          
          def run(arg1=0,arg2=0):
              tg_data = get_target_groups_data()
              tg_data_with_ids = fill_instances_ids(tg_data)
              save_to_csv(tg_data_with_ids, full_file_path)
          #    create_bucket(bucket_name)
              save_to_bucket(bucket_name, filename)
      Handler: index.run
      Runtime: python3.7
      Role: !GetAtt AuditTGExecutionRole.Arn
  AuditTGScheduledRule:
    Type: AWS::Events::Rule
    Properties:
      Description: Scheduled Rule
      ScheduleExpression: cron(0 7 ? * * *)
      State: ENABLED
      Targets:
        - Arn: !GetAtt AuditTGFunction.Arn
          Id: TargetFunctionV1
  AuditTGFunctionPermissionsCF:
    Type: AWS::Lambda::Permission
    Properties:
      Action: 'lambda:InvokeFunction'
      FunctionName: !GetAtt AuditTGFunction.Arn
      Principal: cloudformation.amazonaws.com
  AuditTGFunctionPermissionsEV:
    Type: AWS::Lambda::Permission
    Properties:
      Action: 'lambda:InvokeFunction'
      FunctionName: !GetAtt AuditTGFunction.Arn
      Principal: events.amazonaws.com
      SourceArn: !GetAtt AuditTGScheduledRule.Arn
  AuditTGFunctionLogsGroup:
    Type: AWS::Logs::LogGroup
    DependsOn: AuditTGFunction
    DeletionPolicy: Delete
    Properties:
      LogGroupName: !Join ['/', ['/aws/lambda', !Ref AuditTGFunction]]
      RetentionInDays: 14
  AuditTG:
    Type: AWS::CloudFormation::Macro
    Properties:
      Name: !Sub 'AuditTG'
      Description: Processes inline python in templates
      FunctionName: !GetAtt AuditTGFunction.Arn
  
